[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Connecting to the Database",
    "section": "",
    "text": "Load packages\n\nlibrary(DBI)\nlibrary(RPostgres)\nlibrary(dplyr)\n\nDatabase information\n\n# establish connection with postgres data base\ncon <- dbConnect(RPostgres::Postgres(),dbname = 'postgres',\n      host = 'localhost',\n      port = 5432,\n      user = 'postgres',\n      password = 'vannah') \n\nMake query\n\nres <- dbSendQuery(con, \"\n                   SELECT *\n                   FROM tweetdata AS t1\n                   LEFT JOIN tweettext AS t2\n                   ON t1.id_str = t2.id_str\n                   LIMIT 100;\")\n\nExecute query\n\n# execute query\ndf <- dbFetch(res)\n\nClear query and disconnect from database\n\n# clear query\ndbClearResult(res)\n\n# disconnect from database\ndbDisconnect(con)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Twitter Analysis Project",
    "section": "",
    "text": "Use the navigation bar at the top to explore this project.\n\n\nGitHub Profile Link\nProject Repository Link\n\n\n\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "rocket_league.html",
    "href": "rocket_league.html",
    "title": "Twitter Analysis Project",
    "section": "",
    "text": "Load libraries\n\nlibrary(tidyverse)\nlibrary(rtweet)\n\nAuthenticate Twitter account\n\nauth_setup_default()\nauth_has_default()\n\n[1] TRUE\n\n\nSearch tweets 1000 per execution\n\ndf <- search_tweets(\"rocket league\", n = 1000, include_rts = FALSE, lang = \"en\")\n\n\n\n\nSelect and divide columns\n\ndf1 <- df %>%\n  select(id_str, retweet_count, favorite_count,  created_at)\n\ndf2 <- df %>%\n  select(id_str, full_text, display_text_range, text)\n\nExport as .csv\n\nwrite_csv(df1, \"tweet_data.csv\", col_names = FALSE)\nwrite_csv(df2, \"tweet_text.csv\", col_names = FALSE)\n\nAll data then put into postgreSQL database\n\n\n\nLoad packages\n\nlibrary(DBI)\nlibrary(RPostgres)\nlibrary(dplyr)\n\nDatabase information\n\n# establish connection with postgres data base\ncon <- dbConnect(RPostgres::Postgres(),dbname = 'postgres',\n      host = 'localhost',\n      port = 5432,\n      user = 'postgres',\n      password = 'vannah') \n\nMake query\n\nres <- dbSendQuery(con, \"\n                   SELECT *\n                   FROM tweetdata AS t1\n                   LEFT JOIN tweettext AS t2\n                   ON t1.id_str = t2.id_str\n                   LIMIT 100;\")\n\nExecute query\n\n# execute query\ndf <- dbFetch(res)\n\nClear query and disconnect from database\n\n# clear query\ndbClearResult(res)\n\n# disconnect from database\ndbDisconnect(con)"
  },
  {
    "objectID": "tweet_scrape.html",
    "href": "tweet_scrape.html",
    "title": "Twitter Analysis Project",
    "section": "",
    "text": "In order to get the Tweets I will first need to load the packages tidyverse and rtweet.\n\nlibrary(tidyverse)\nlibrary(rtweet)\n\nNext I can authenticate my Twitter developer account.\n\nauth_setup_default()\nauth_has_default()\n\n[1] TRUE\n\n\n\n\nFinally I will then search and scrape tweets that contain ‘rocket league’, I am doing 1000 Tweets at a time, that do not include Retweets and are in English.\n\ndf <- search_tweets(\"rocket league\", n = 1000, include_rts = FALSE, lang = \"en\")\n\n\n\nNow I will clean up the dataset into two more manageable tables that will be imported in into my postgreSQL database.\n\ndf1 <- df %>%\n  select(id_str, retweet_count, favorite_count,  created_at)\n\ndf2 <- df %>%\n  select(id_str, full_text, display_text_range, text)\n\n\n\n\nAfter creating the two tables, I will export them in .csv format. I will then get on pgadmin4 and import the .csv’s into the database.\n\nwrite_csv(df1, \"tweet_data.csv\", col_names = FALSE)\nwrite_csv(df2, \"tweet_text.csv\", col_names = FALSE)\n\n\n\n\n\n\npdf <- search_tweets(\"pokemon go\", n = 1000, include_rts = FALSE, lang = \"en\")\n\n\n\n\npdf1 <- pdf %>%\n  select(id_str, retweet_count, favorite_count,  created_at)\n\npdf2 <- pdf %>%\n  select(id_str, full_text, display_text_range, text)\n\n\n\n\n\nwrite_csv(pdf1, \"pogo_tweet_data.csv\", col_names = FALSE)\nwrite_csv(pdf2, \"pogo_tweet_text.csv\", col_names = FALSE)"
  },
  {
    "objectID": "database.html",
    "href": "database.html",
    "title": "Twitter Analysis Project",
    "section": "",
    "text": "First I loaded the DBI, RPostgres, and dplyr packages which will allow me to connect to the database and query it.\n\nlibrary(DBI)\nlibrary(RPostgres)\nlibrary(dplyr)\n\nNext I will set the parameters to connect to my local database.\n\n# establish connection with postgres data base\ncon <- dbConnect(RPostgres::Postgres(),dbname = 'postgres',\n      host = 'localhost',\n      port = 5432,\n      user = 'postgres',\n      password = 'vannah') \n\nOnce connected I can construct a query.\n\nres <- dbSendQuery(con, \"\n                   SELECT *\n                   FROM tweetdata AS t1\n                   LEFT JOIN tweettext AS t2\n                   ON t1.id_str = t2.id_str\n                   LIMIT 100;\")\n\nAnd the execute the query.\n\n# execute query\ndf <- dbFetch(res)\n\nAfter I am done with using the database I will clear out the query and disconnect from the database in order so save resources.\n\n# clear query\ndbClearResult(res)\n\n# disconnect from database\ndbDisconnect(con)"
  }
]